# ===================================================================
# Tool-Based Chat Agent Master Configuration
# ===================================================================
# This configuration is designed for an agent that uses a single powerful
# LLM to orchestrate calls to a predefined set of tools.

agent_name: "আশা"
agent_story: "আমি বাংলাদেশ সরকারের নাগরিকদের সহায়তা করার জন্য ডিজাইন করা একটি ডিজিটাল উদ্যোগ। আমার লক্ষ্য হলো সরকারি সেবা সংক্রান্ত তথ্য আপনার জন্য সহজলভ্য করে তোলা। jiggasha.ai এর মাধ্যমে, আমি আপনাকে বিভিন্ন সরকারি সেবা, নীতিমালা, এবং প্রক্রিয়া সম্পর্কে তথ্য প্রদান করতে পারি। আমি আপনার প্রশ্নের উত্তর দিতে, নির্দেশনা প্রদান করতে, এবং প্রাসঙ্গিক তথ্য খুঁজে পেতে সাহায্য করতে প্রস্তুত। আসুন, একসাথে বাংলাদেশের নাগরিক সেবার উন্নয়নে কাজ করি!"

# --- Primary LLM Service Definition ---
# The single, orchestrating LLM that will handle reasoning and tool calls.
llm_service:
  name: "primary_llm"
  api_key_env: "VLLM_API_KEY"
  model_name_env: "VLLM_MODEL_NAME"
  base_url_env: "VLLM_BASE_URL"
  max_context_tokens: 32000 # Max context window for the model.

# --- Conversation Management ---
conversation:
  history_window: 10  # Number of past user/AI exchanges to retain in memory.

# --- Vector Retriever Configuration (for the 'retrieve_knowledge' tool) ---
vector_retriever:
  top_k: 10 # Number of initial candidates to retrieve from each ChromaDB collection.
  collections:
    - "PropositionsDB"
    - "SummariesDB"
    - "QuestionsDB"
  max_passages_to_select: 3 # Final number of passages to return after RRF fusion.
  # Reciprocal Rank Fusion (RRF) constant. A standard value is 60.
  # Lower values prioritize top-ranked items more heavily.
  rrf_k: 60
  # The key in the vector metadata that stores the unique passage identifier.
  passage_id_meta_key: "passage_id"

# --- Token Management for Prompt Construction ---
# Manages how the final prompt is built to avoid exceeding the context limit.
token_management:
  # The model name (from an environment variable) to use for tokenization.
  # This should match the primary LLM to ensure accurate token counting.
  tokenizer_model_name_env: "VLLM_MODEL_NAME"
  # A fixed number of tokens reserved for the master system prompt's instructions,
  # ensuring that dynamic content (like history) doesn't push it out of context.
  prompt_template_reservation_tokens: 1024
  # The percentage of the remaining token budget to allocate for conversation history.
  # Here, 40% of the available space (after reserving for the template and query)
  # will be used for history. The rest is for tool results.
  history_truncation_budget: 0.4

# --- LLM Call Parameters ---
# Default generation parameters for the primary LLM's final response, after
# it has received the results from any tool calls.
llm_call_parameters:
  temperature: 0.1
  max_tokens: 2048 # Max tokens for the final generated answer.

# --- Response Templates ---
# Standardized text for specific scenarios handled by the agent's logic.
response_templates:
  error_fallback: "একটি প্রযুক্তিগত ত্রুটির কারণে আমি এই মুহূর্তে সাহায্য করতে পারছি না। অনুগ্রহ করে কিছুক্ষণ পর আবার চেষ্টা করুন।"
  tool_failure: "একটি প্রযুক্তিগত সমস্যার কারণে আমি এই মুহূর্তে তথ্য যাচাই করতে পারছি না। অনুগ্রহ করে কিছুক্ষণ পর আবার চেষ্টা করুন।"
  no_passages_found: "দুঃখিত, আমি আপনার অনুরোধ সম্পর্কিত কোনো তথ্য আমার জ্ঞানভান্ডারে খুঁজে পাচ্ছি না।"