# ===================================================================
# Tool-Based Chat Agent Master Configuration
# ===================================================================
# This configuration is designed for an agent that uses a single powerful
# LLM to orchestrate calls to a predefined set of tools.

agent_name: "meaty"
agent_story: "বেঙ্গল মিট-এর পক্ষ থেকে আপনার ব্যক্তিগত সহকারী। নিরাপদ ও স্বাস্থ্যকর মাংস খুঁজে পেতে, সেরা অফারগুলো জানতে এবং আপনার যেকোনো প্রশ্নের উত্তর দিতে আমি এখানে আছি। আসুন, আপনার প্রয়োজন অনুযায়ী সেরা পণ্যটি বেছে নিতে আমি আপনাকে সাহায্য করি।"

# --- Primary LLM Service Definition ---
# The single, orchestrating LLM that will handle reasoning and tool calls.
llm_service:
  name: "primary_llm"
  api_key_env: "VLLM_API_KEY"
  model_name_env: "VLLM_MODEL_NAME"
  base_url_env: "VLLM_BASE_URL"
  max_context_tokens: 32000 # Max context window for the model.

# --- Conversation Management ---
conversation:
  history_window: 10  # Number of past user/AI exchanges to retain in memory.

# --- Vector Retriever Configuration (for the 'retrieve_knowledge' tool) ---
vector_retriever:
  top_k: 10 # Number of initial candidates to retrieve from each ChromaDB collection.
  collections:
    - "PropositionsDB"
    - "SummariesDB"
    - "QuestionsDB"
  max_passages_to_select: 3 # Final number of passages to return after RRF fusion.
  # Reciprocal Rank Fusion (RRF) constant. A standard value is 60.
  # Lower values prioritize top-ranked items more heavily.
  rrf_k: 60
  # The key in the vector metadata that stores the unique passage identifier.
  passage_id_meta_key: "passage_id"

# --- Token Management for Prompt Construction ---
# Manages how the final prompt is built to avoid exceeding the context limit.
token_management:
  # The model name (from an environment variable) to use for tokenization.
  # This should match the primary LLM to ensure accurate token counting.
  tokenizer_model_name_env: "VLLM_MODEL_NAME"
  # A fixed number of tokens reserved for the master system prompt's instructions,
  # ensuring that dynamic content (like history) doesn't push it out of context.
  prompt_template_reservation_tokens: 1024
  # The percentage of the remaining token budget to allocate for conversation history.
  # Here, 40% of the available space (after reserving for the template and query)
  # will be used for history. The rest is for tool results.
  history_truncation_budget: 0.4

# --- LLM Call Parameters ---
# Default generation parameters for the primary LLM's final response, after
# it has received the results from any tool calls.
llm_call_parameters:
  temperature: 0.1
  max_tokens: 2048 # Max tokens for the final generated answer.

# --- Response Templates ---
# Standardized text for specific scenarios handled by the agent's logic.
response_templates:
  error_fallback: "একটি প্রযুক্তিগত ত্রুটির কারণে আমি এই মুহূর্তে সাহায্য করতে পারছি না। অনুগ্রহ করে কিছুক্ষণ পর আবার চেষ্টা করুন।"
  tool_failure: "একটি প্রযুক্তিগত সমস্যার কারণে আমি এই মুহূর্তে তথ্য যাচাই করতে পারছি না। অনুগ্রহ করে কিছুক্ষণ পর আবার চেষ্টা করুন।"
  no_passages_found: "দুঃখিত, আমি আপনার অনুরোধ সম্পর্কিত কোনো তথ্য আমার জ্ঞানভান্ডারে খুঁজে পাচ্ছি না।"